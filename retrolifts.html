<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Retrolifts XLab | Prakriti Prasad</title>
  <link rel="stylesheet" href="retrolifts.css"/>
  <style>
    body {
      display: flex;
      min-height: 100vh;
      font-family: "Poppins", sans-serif;
    }

    .sidebar {
      width: 250px;
      background: #1a3a70;
      color: #fff;
      padding: 2rem 1rem;
    }

    .sidebar h2 {
      font-size: 1.5rem;
      margin-bottom: 2rem;
      text-align: center;
    }

    .sidebar nav {
      display: flex;
      flex-direction: column;
      gap: 1rem;
    }

    .sidebar nav a {
      color: #fff;
      text-decoration: none;
      font-weight: 500;
    }

    .sidebar nav a:hover {
      text-decoration: underline;
    }

    .content {
      flex: 1;
      padding: 2rem 4rem;
      background: #f9f9fb;
    }

    .content section {
      margin-bottom: 4rem;
    }

    .content h1 {
      color: #1a3a70;
      margin-bottom: 1rem;
    }

    .content p {
      color: #333;
      line-height: 1.7;
    }

    .content img {
      max-width: 40%;
      height: auto;
      display: block;
      margin: 1rem 0;
      border-radius: 8px;
    }
  </style>
</head>

<body>
  <aside class="sidebar">
    <h2>Retrolifts XLab</h2>
    <nav>
      <a href="#about">About This Project</a>
      <a href="#camera">Camera Setup</a>
      <a href="#distances">Getting Object Distances</a>
      <a href="#isaac">Isaac Sim</a>
      <a href="#codes">Path Planning in Isaac Sim</a>
    </nav>
  </aside>

  <main class="content">
    <section id="about">
      <h1>About This Project</h1>
      <p>
        Motivation: AMRs cannot adapt well to existing warehouses, resulting in sub-optimal performance in mixed-use environments where automated and manual operations must coexist.
        A dedicated supervisor is required to troubleshoot disengagement. This project will retrofit existing drive-by-wire electric forklifts to enable gradual adoption from manual to up to 60% autonomous operation.
        We are going to approach this problem in three steps.
      </p>
      <p>
        Our answer is to build on existing platforms using an infrastructure-based computer vision system. This system guides fleets of forklifts for end-to-end cross-dock operations using a bird's-eye view.
        The cameras are mounted on ceilings.
      </p>
      <p>
        Below is an overview of our solution:
      </p>
      <img src="./retrolift/solution.png" alt="Solution Overview">

      <p>
        The diagram below shows a working pipeline:
      </p>
      <img src="./retrolift/pipeline.png" alt="Pipeline Diagram">

      <p>
        NVIDIA Isaac Sim provides an environment to develop and test a vision-based localization system before deploying it in the real world.
        The workflow involves detecting forklifts within video streams, localizing the forklifts based on detection information, and replicating the setup in a real environment (after camera calibration and testing).
        The project is divided into several sections, each focusing on a specific aspect of the system.
      </p>
      <p>
        This will be expanded in the future sections. We first start with the basics: how to set up the camera.
      </p>
    </section>

    <section id="camera">
      <h1>Camera Setup</h1>
      <p>
        We have four cameras with the following specifications (for full details, please refer to the product page): 
        <a href="https://www.baslerweb.com/en-us/shop/a2a1920-51gcpro/" target="_blank">Basler A2A1920-51gcPRO</a>.
      </p>
      <p>
        <strong>Specifications:</strong><br>
        Resolution (H × V): 1920 px × 1200 px<br>
        Resolution (MP): 2.3 MP<br>
        Pixel Size (H × V): 3.45 μm × 3.45 μm
      </p>
      <p>
        Each camera connects to a port on the TP-Link 5-Port Gigabit Easy Smart Switch with 4-Port PoE+ using a 4-meter Ethernet cable. 
        The switch includes a USB port to connect the network to the computer (in our case, an ASUS desktop) and is powered by a dedicated power supply.
        Refer to the images below to locate and connect all components. Once identified, position each camera on a tripod or ceiling mount as needed.
      </p>
      <p>
        Make the connections as shown in the picture. The blue cable connects the camera to the switch, and the white cable connects the switch to the computer. 
        The power cable for the Ethernet switch is labeled <em>“Retrolifts POE”</em>. Connect the power cable and switch the device on.
      </p>
    
      <!-- insert pictures -->
      <img src="retrolift/ethernet_switch.jpg" alt="Ethernet Switch">
    
      <p>
        Mount each camera securely on a tripod or ceiling mount as shown below.
      </p>
    
      <img src="retrolift/camera_setup.jpg" alt="Camera Setup">
    
      <p>
        Once all cameras are mounted and connected, open the Pylon Viewer software. 
        A short GIF is included to demonstrate how to power on the system and optimize the bandwidth settings.
        The computer-to-switch connection supports up to 125 MBps, and each individual port also supports up to 125 MBps.
        Therefore, when using all four cameras simultaneously, adjust the bandwidth allocation in the Pylon Viewer software, accordingly—for example, 30 Mbps per camera (it usually does this automatically once we press the start button).
      </p>
      <p>
        Adjust the aperture and focus for each camera as needed. Once setup is complete, run the script (<em>insert script name</em>) and follow the README instructions to perform the calibration.
      </p>
      <p>
        Below is the ChArUco board used for camera calibration:
      </p>
      <img src="retrolift/charuco.png" alt="ChArUco Board">
    
      <p>
        The following subsection explains how to perform the camera calibration.<br>
        <strong>1.1 Camera Calibration:</strong>
      </p>
      
      <ul>
        <li>On the ASUS desktop or in the GitHub repository, navigate to the README file: {insert link here}.</li>
        <li>Run the script to capture videos. While capturing data move the ChArUco board. Pass this video to the python script to extract frames. Set the frame interval accordingly.</li>
        <li>Locate the folder where these images are saved and provide this folder path to the calibration script.</li>
        <li>Ensure that each camera is properly calibrated. Follow the README documentation carefully.</li>
        <li>For the theoretical background, refer to the OpenCV ArUco documentation: <a href="https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html" target="_blank">OpenCV documentation</a>.</li>
        <li>To learn how to use a ChArUco board with OpenCV, see this guide: <a href="https://medium.com/@ed.twomey1/using-charuco-boards-in-opencv-237d8bc9e40d" target="_blank">Using ChArUco Boards in OpenCV</a>.</li>
      </ul>

      <p>
        <strong>1.2 AprilTag Detection:</strong>
    
      </p>

      <ul>
        Follow the ReadME file to run the AprilTag detection script.
      </ul>


      <!-- some important notes -->
      <p>
        <strong>Important Notes:</strong>
        <ul>
          <li>If the camera is being accessed by Pylon Viewer, it cannot be accessed by scripts at the same time. Close one of them to run the camera.</li>
          <li>If the first script runs successfully that would mean the camera is in focus.</li>
          <li>Incase you want to use a different ChArUco board in the future define it in the script. It is hardcoded right now.</li>
        </ul>


    </section>
    




    <section id="distances">
      <h1>Distance/Depth Estimation</h1>
    
      <p>
        The next step is to estimate the distance of the object from the camera. This is done using NVIDIA's CenterPose model.
        You can find the model repository 
        <a href="https://github.com/NVlabs/CenterPose" target="_blank">here</a>.
        The related paper, <em>“Keypoint-Based Category-Level Object Pose Tracking from an RGB Sequence with Uncertainty Estimation,”</em> introduces <strong>CenterPoseTrack</strong> —
        a novel framework for category-level 6-DoF object pose tracking using only a monocular RGB video stream, without requiring 3D CAD models of specific object instances.
      </p>
    
      <p>
        Unlike traditional instance-level pose tracking, which assumes a known 3D model for each object, this method generalizes to entire categories 
        (e.g., cereal boxes, mugs) with varying shapes and sizes. The system jointly detects and tracks an object’s 3D position, orientation, and relative dimensions over time.
      </p>
    
      <p><strong>Key contributions:</strong></p>
      <ul>
        <li>Uses a <strong>keypoint-based representation</strong>: a deep network predicts 2D image coordinates of the 3D cuboid’s corners (keypoints) along with their uncertainties.</li>
        <li>Introduces a <strong>tracklet-conditioned network</strong> that uses the previous frame’s prediction and uncertainty heatmaps as input, improving temporal consistency.</li>
        <li>Applies a <strong>probabilistic filtering process</strong> combining Bayesian filtering and Kalman filtering to fuse information over time, reduce jitter, and handle occlusions.</li>
        <li>Explicitly estimates <strong>uncertainty</strong> in both keypoint locations and object dimensions, which helps stabilize predictions frame to frame.</li>
        <li>Computes the final pose using a <strong>PnP (Perspective-n-Point)</strong> algorithm with the filtered keypoints and dimensions.</li>
      </ul>
    
      <h2>How to Run the CenterPose Model</h2>
      <ol>
        <li>Go to the CenterPose folder in your Documents, or clone the repository if you haven’t already.</li>
        <!-- Example: <code>git clone https://github.com/NVlabs/CenterPose.git</code> -->
        <li>Open the <code>notex.txt</code> file. This file serves as the key guide for running the repository.</li>
        <li>Run line 2 in the inference section. A screenshot is provided for reference below:</li>
      </ol>
      <img src="retrolift/centerpose.png" alt="CenterPose Inference">
    
      <ol start="4">
        <li>The model is loaded from the <code>models</code> folder.</li>
        <li>The input will be image frames, located in <code>input/files</code>.</li>
        <li>The results are saved in the <code>inf_results</code> folder. Note: check <code>input_files</code> inside <code>inference</code>. (The purpose of the <code>outputs</code> folder outside <code>inference</code> is currently unclear.)</li>
      </ol>
    
      <p>
        A typical detection output looks like this:
      </p>
      <img src="retrolift/000000_out_img_pred.png" alt="CenterPose Output">
    
      <h2>Important Notes</h2>
      <ul>
        <li>Check the JSON file in <code>inference/results</code>. If the <code>kps_displacement_mean</code> has a value, it indicates that the object (e.g., forklift) was recognized and the value is stored in the matrix. The value is the center of the base of the box detected.</li>
        <li>The current model was trained on synthetic data generated using NVIDIA’s Replicator. The ground truth of every forklift must be known.</li>
        <li>The tracking component described in the paper was not implemented in this setup.</li>
        <li> As the model is not trained on objects that are far away, it might not be the best solution.</li>
      </ul>
    
      <h2>Performance on the Retrolifts Dataset</h2>
      <p>
        This report — <a href="https://docs.google.com/document/d/15rteuPFY45iXWRHcHasuSMUGSRGU74XH/edit" target="_blank">View Report</a> — contains detailed results of running the CenterPose model on the Retrolifts dataset.
      </p>
      <p>
        In summary, the Retrolift object was detected well at certain angles but poorly at others. For example:
      </p>
    
      <p>
        <strong>At 0 degrees:</strong> the model detected the object accurately. Blue dots represent ground truth; orange dots represent detections.
      </p>
      <img src="retrolift/zero_deg_rot.png" alt="CenterPose Detection at 0 Degrees">
    
      <p>
        <strong>At 90 degrees:</strong> the detection performance dropped significantly, with sparse or missing detections (orange dots).
      </p>
      <img src="retrolift/ninety_deg_rot.png" alt="CenterPose Detection at 90 Degrees">
    
    </section>
    



    <section id="isaac">
      <h1>Isaac Sim</h1>
      <p>
        Isaac Sim has four related components essential for this project:
      </p>
      <ul>
        <li>Isaac Lab</li>
        <li>Omniverse Launcher</li>
        <li>Isaac Gym</li>
        <li>Isaac Replicator</li>
      </ul>
    
      <ul>
        <li><strong>Omniverse Launcher:</strong> Omniverse is a platform of applications built on USD.</li>
        <li><strong>Isaac Sim:</strong> Isaac Sim is a specific application of Omniverse for Robotics.</li>
        <li><strong>Isaac Lab:</strong> Isaac Lab is a tool for training. Isaac Lab replaces Isaac Gym (which was also a training environment).</li>
        <li><strong>We build in Isaac Gym and train it on Isaac Lab.</strong></li>
        <li>Above is a summary of this YouTube video: <a href="https://www.youtube.com/watch?v=_kzW6yBno6Q" target="_blank">Extremely useful video for “how to enable scripting” — not mentioned in the documentation.</a></li>
      </ul>

      To arrive at developing the warehouse environment we first need to get familiar with Isaac sim. Here are the steps in the documentation which could help in it:
      <ul>
        <li>Install the Omniverse Launcher and Isaac Sim.</li>
        <li>Follow the documentation to set up the environment: <a href="https://docs.omniverse.nvidia.com/app_isaacsim/latest/isaac_sim_setup.html" target="_blank">Isaac Sim Setup</a>.</li>
        <li>Learn how to create a simple scene in Isaac Sim: <a href="https://docs.omniverse.nvidia.com/app_isaacsim/latest/tutorials/creating_a_simple_scene.html" target="_blank">Creating a Simple Scene</a>.</li>
        <li>Explore the Isaac Lab documentation for training environments: <a href="https://docs.omniverse.nvidia.com/app_isaacsim/latest/isaac_lab.html" target="_blank">Isaac Lab Documentation</a>.</li>
      </ul>

    </section>

    





    <section id="codes">
      <h1>Path Planning in Isaac Sim</h1>
    
      <img src="media/auto_1080p.gif" alt="Autonomous demo" />
      <img src="media/click_pick_place_1080p.gif" alt="Click, pick & place demo" />
    


    
      <h3>Installation Instructions</h3>
      <ol>
  
        <li>Navigate to the Isaac Sim directory:<br>
          Launch the <code>Omniverse Launcher</code>, under <strong>LIBRARY</strong>, launch <code>Isaac Sim</code>, and click <strong>Open in Terminal</strong>. This will open a terminal in the Isaac Sim directory: <code>~/.local/share/ov/pkg/isaac_sim-2023.1.1</code>.
        </li>
        <li>Clone this repository into the Isaac Sim directory:<br>
          <code>git clone https://github.com/mlab-upenn/autonomous_forklift_teleop.git</code>
        </li>
        <li>In the terminal at the Isaac Sim directory, type <code>code .</code> to launch Visual Studio Code in this directory.<br>
          In VS Code, open <code>autonomous_teleop.py</code> under <code>ISAAC_SIM-2023.1.1/autonomous_forklift_teleop/</code> and update the <code>PC name/user</code> in <code>MAP_PATH</code> to match your local system.
        </li>
        <li>Install <code>opencv-python</code> in the Isaac Sim environment using: (Do not install opencv using sudo apt)<br>
          <code>./python.sh -m pip install opencv-python</code>
        </li>
        <li>Download and unzip the asset file <code>forklift_teleop</code> from 
          <a href="https://drive.google.com/file/d/1DxOdrLoFRdGp0uXRDlzmdpMpA50Til8i/view?usp=sharing" target="_blank">Google Drive</a>. 
          Upload the unzipped folder to NUCLEUS at <code>omniverse://localhost/Projects/</code> using the <strong>NUCLEUS</strong> tool in Omniverse Launcher.
        </li>
      </ol>
    
      <h3>Running the Autonomous Forklift Teleop Demo</h3>
      <ol>
        <li>Launch <code>Omniverse Launcher</code> and start <code>Isaac Sim</code>. Click <strong>Open in Terminal</strong>, then type <code>code .</code> in the terminal.</li>
        <li>Open <code>autonomous_teleop.py</code> in Visual Studio Code.</li>
        <li>Run the demo using the <strong>Run and Debug</strong> tab or press <code>F5</code>.</li>
        <em> Note:</em> If you face an error there are two possible reasons:
        You are running it in Isaac sim 4.5. This code runs with Isaac Sim 4.2 as there are dependency issues.
        YOu are not inside the root folder
        Omniverse has trouble launching. If you fae this follow steps to reolve this in Troubleshooting omniverse issues. 
              
        <li>Isaac Sim will launch and load the warehouse scene and top-down map.<br>
          <em>Note:</em> The first launch may take 5–10 minutes to compile ray tracing shaders. Some non-critical error messages may appear.
        </li>
        <li>Switch the camera view from <strong>Perspective</strong> to <strong>bot_left</strong> or any pre-set view. Additional cameras are attached to forklifts for first-person views.</li>
        <li>By default, the program starts in <strong>Manual mode</strong>:<br>
          Use <code>W, A, S, D</code> to move forklift #2.<br>
          Use <code>E</code> and <code>Q</code> to adjust the fork height.<br>
          Use <code>C</code> to reset the pose.<br>
          Use <code>Z</code> to switch to <strong>Autonomous mode</strong>. Five forklifts will begin pre-programmed pallet lift and place sequences. The map will show forklifts' current poses, goal poses, and trajectories.<br>
          Use <code>X</code> to return to Manual mode.
        </li>
      </ol>
    
      This is the first thing that you should see if everything was done successfully:
      <img src="retrolift/stage.png" alt="Isaac Sim" />


      <h2>Steps to Debug</h2>
      
        <h4>Troubleshooting Omniverse Issues</h4>
        <ul>
          <li>Go to NUCLEUS and press on the hamburger icon. There press the uninstall button. </li>
          <li>Once done,click on add service. Follow the video below. (Please ignore the meeting will end popup)</li>
        </ul>
        <!-- add a video below -->
        <video controls width="640">
          <source src="retrolift\omniverse_fix.mp4" type="video/mp4">
        </video>
    
        

        <h4>Steps to Resolve Code Execution Problems</h4>
        <ul>
          <li>Revert to Isaac Sim version 4.2 if the current version has compatibility issues.</li>
          <li>Perform a fresh installation of Isaac Sim 4.2 if reverting doesn't resolve the issue.</li>
          <li>Verify that you are running the code from the root folder of the Isaac Sim directory (e.g., the "retrolifts" folder should be inside the Isaac Sim 4.2 folder).</li>
        </ul>





      <!-- <h2>Map Editor</h2>
      <p>Refer to the <code>map_editor</code> folder inside the <code>map_editor</code> branch.</p> -->

      <!-- <h3>Running the Data Collection Pipeline for Cameras</h3>
      <p>Refer to the <code>camera_scripts</code> folder inside the <code>camera_scripts</code> branch.</p>
      <p>Data from the Raymond SEC visit on 1st May is stored in the home directory of the machine in the folder titled <strong>RetroLifts_05-01</strong>.</p>
     -->


      <h4>Useful Links and Tutorials</h4>
      <ul>
        <li><a href="https://docs.omniverse.nvidia.com/isaacsim/latest/features/physics/index.html" target="_blank">NVIDIA Isaac Sim Documentation</a></li>
        <li><a href="https://docs.omniverse.nvidia.com/isaacsim/latest/replicator_tutorials/tutorial_replicator_getting_started.html?highlight=Semantic#the-semantics-schema-editor" target="_blank">Enable Camera Feed with 2D/3D BBoxes and Semantic Info</a></li>
        <li><a href="https://docs.omniverse.nvidia.com/isaacsim/latest/features/mapping/ext_omni_isaac_occupancy_map.html" target="_blank">Generate Occupancy Map of the Warehouse Scene</a></li>
        <li><a href="https://docs.omniverse.nvidia.com/isaacsim/latest/features/physics/index.html#" target="_blank">Add Physics to Objects</a></li>
        <li><a href="https://openusd.org/release/api/index.html" target="_blank">USD API Documentation</a></li>
      </ul>


    </section>
    





  </main>
</body>
</html>
